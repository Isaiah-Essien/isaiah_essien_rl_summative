{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQkuitsvMAmd5hKab/Vn4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Isaiah-Essien/isaiah_essien_rl_summative/blob/main/seizure_rl_simulator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Epilepsy Detection RL-Simultion using DQN and PPO\n",
        "\n",
        "This project aims to simulate a rienforcement learning approach to my mission-based project of Epilepsy detection and care system.\n",
        "\n",
        "It simulates an agent(Camera, yellow color) and a patient(green/red dot) randomly performing action in a 10 by 10 action space, with each grid being an action with a reward or penalty.\n",
        "\n"
      ],
      "metadata": {
        "id": "nNJ1Yhk_hZrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "3Ob4PzCaHNb1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================Imports=================\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pygame\n",
        "from pygame.locals import *\n",
        "from OpenGL.GL import *\n",
        "from stable_baselines3 import DQN\n",
        "from OpenGL.GLU import *\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "metadata": {
        "id": "TmvSKKvABpBe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============Static rendering===========\n",
        "\n",
        "grid_size = 10\n",
        "cell_size = 1\n",
        "\n",
        "def draw_grid():\n",
        "    glColor3f(0.8, 0.8, 0.8)\n",
        "    glBegin(GL_LINES)\n",
        "    for i in range(grid_size + 1):\n",
        "        glVertex3f(i * cell_size, 0, 0)\n",
        "        glVertex3f(i * cell_size, grid_size * cell_size, 0)\n",
        "        glVertex3f(0, i * cell_size, 0)\n",
        "        glVertex3f(grid_size * cell_size, i * cell_size, 0)\n",
        "    glEnd()\n",
        "\n",
        "def draw_patient(patient_pos, seizure):\n",
        "    if seizure:\n",
        "        glColor3f(1.0, 0.0, 0.0)  # Red if seizure\n",
        "    else:\n",
        "        glColor3f(0.0, 1.0, 0.0)  # Green if no seizure\n",
        "    glPointSize(10)\n",
        "    glBegin(GL_POINTS)\n",
        "    glVertex3f(patient_pos[0] + 0.5, patient_pos[1] + 0.5, 0)\n",
        "    glEnd()\n",
        "\n",
        "def draw_camera(camera_pos):\n",
        "    glColor3f(0.0, 0.0, 1.0)  # Blue for camera\n",
        "    glPointSize(15)\n",
        "    glBegin(GL_POINTS)\n",
        "    glVertex3f(camera_pos[0] + 0.5, camera_pos[1] + 0.5, 0)\n",
        "    glEnd()\n",
        "\n",
        "def visualize_environment(patient_pos, camera_pos, seizure):\n",
        "    pygame.init()\n",
        "    display = (600, 600)\n",
        "    pygame.display.set_mode(display, DOUBLEBUF | OPENGL)\n",
        "    gluOrtho2D(0, grid_size, 0, grid_size)\n",
        "\n",
        "    glClearColor(1, 1, 1, 1)\n",
        "    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n",
        "\n",
        "    draw_grid()\n",
        "    draw_patient(patient_pos, seizure)\n",
        "    draw_camera(camera_pos)\n",
        "\n",
        "    pygame.display.flip()\n",
        "\n",
        "    # Keep window open for static visualization\n",
        "    running = True\n",
        "    while running:\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT:\n",
        "                running = False\n",
        "    pygame.quit()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    patient_pos = np.array([4, 6])\n",
        "    camera_pos = np.array([5, 5])\n",
        "    seizure = True  # or False to visualize both cases\n",
        "    visualize_environment(patient_pos, camera_pos, seizure)\n"
      ],
      "metadata": {
        "id": "YETTCRbnBsPj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OLXuOF_FAuOC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = {\n",
        "    'bg': (25, 25, 112),\n",
        "    'grid': (70, 130, 180),\n",
        "    'patient_normal': (60, 179, 113),\n",
        "    'patient_seizure': (220, 20, 60),\n",
        "    'drone': (255, 215, 0),\n",
        "}\n",
        "\n",
        "class EpilepsyDetectionEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.grid_size = 10\n",
        "        self.camera_pos = np.array([5, 5])\n",
        "        self.patient_pos = np.array([np.random.randint(0, 10), np.random.randint(0, 10)])\n",
        "        self.camera_zoom = 1\n",
        "        self.seizure = False\n",
        "        self.time_step = 0\n",
        "\n",
        "        self.action_space = spaces.Discrete(9)\n",
        "        self.observation_space = spaces.Box(low=0, high=10, shape=(5,), dtype=np.float32)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.window_size = 700\n",
        "        self.cell_size = self.window_size // self.grid_size\n",
        "\n",
        "        if render_mode == \"rgb_array\":\n",
        "            pygame.init()\n",
        "            self.window = pygame.Surface((self.window_size, self.window_size))\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.camera_pos = np.array([5, 5])\n",
        "        self.patient_pos = np.array([np.random.randint(0, 10), np.random.randint(0, 10)])\n",
        "        self.camera_zoom = 1\n",
        "        self.seizure = False\n",
        "        self.time_step = 0\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return np.array([\n",
        "            self.camera_pos[0], self.camera_pos[1],\n",
        "            self.camera_zoom,\n",
        "            self.patient_pos[0], self.patient_pos[1]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = -0.1\n",
        "        self.time_step += 1\n",
        "\n",
        "        self.patient_pos += np.random.choice([-1, 0, 1], size=2)\n",
        "        self.patient_pos = np.clip(self.patient_pos, 0, self.grid_size - 1)\n",
        "        self.seizure = np.random.rand() < 0.05\n",
        "\n",
        "        if action == 0: self.camera_pos[0] -= 1\n",
        "        elif action == 1: self.camera_pos[0] += 1\n",
        "        elif action == 2: self.camera_zoom = min(self.camera_zoom + 1, 5)\n",
        "        elif action == 3: self.camera_zoom = max(self.camera_zoom - 1, 1)\n",
        "        elif action == 4: self.camera_pos[1] -= 1\n",
        "        elif action == 5: self.camera_pos[1] += 1\n",
        "        self.camera_pos = np.clip(self.camera_pos, 0, self.grid_size - 1)\n",
        "\n",
        "        if self.seizure and np.array_equal(self.camera_pos, self.patient_pos):\n",
        "            reward += 10\n",
        "        elif self.seizure and not np.array_equal(self.camera_pos, self.patient_pos):\n",
        "            reward -= 10\n",
        "        elif not self.seizure and np.array_equal(self.camera_pos, self.patient_pos):\n",
        "            reward -= 5\n",
        "\n",
        "        done = self.time_step >= 200\n",
        "        return self._get_obs(), reward, done, False, {}\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            self.window.fill(colors['bg'])\n",
        "\n",
        "            for x in range(self.grid_size):\n",
        "                for y in range(self.grid_size):\n",
        "                    rect = pygame.Rect(x*self.cell_size, y*self.cell_size, self.cell_size, self.cell_size)\n",
        "                    pygame.draw.rect(self.window, colors['grid'], rect, 2, border_radius=8)\n",
        "\n",
        "            patient_color = colors['patient_seizure'] if self.seizure else colors['patient_normal']\n",
        "            patient_pos_pix = ((self.patient_pos + 0.5) * self.cell_size).astype(int)\n",
        "            pygame.draw.circle(self.window, patient_color, patient_pos_pix, self.cell_size//3)\n",
        "\n",
        "            drone_pos_pix = ((self.camera_pos + 0.5) * self.cell_size).astype(int)\n",
        "            pygame.draw.rect(self.window, colors['drone'], (*drone_pos_pix - self.cell_size//4, self.cell_size//2, self.cell_size//2), border_radius=10)\n",
        "\n",
        "            for angle in [45, 135, 225, 315]:\n",
        "                offset = np.array([np.cos(np.radians(angle)), np.sin(np.radians(angle))]) * self.cell_size//2.5\n",
        "                rotor_pos = drone_pos_pix + offset.astype(int)\n",
        "                pygame.draw.circle(self.window, colors['drone'], rotor_pos, self.cell_size//10)\n",
        "\n",
        "            return np.transpose(pygame.surfarray.array3d(self.window), axes=(1, 0, 2))\n",
        "\n",
        "        elif self.render_mode == \"human\":\n",
        "            print(f\"Time:{self.time_step}, Camera:{self.camera_pos}, Patient:{self.patient_pos}, Seizure:{self.seizure}\")\n",
        "\n",
        "    def close(self):\n",
        "        pygame.quit()\n"
      ],
      "metadata": {
        "id": "oUcs1YpBSF6N"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#============Logger==================\n",
        "\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "\n",
        "class EpilepsyLoggerCallback(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(EpilepsyLoggerCallback, self).__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        infos = self.locals.get('infos', [])\n",
        "        for info in infos:\n",
        "            if 'episode' in info:\n",
        "                episode_reward = info['episode']['r']\n",
        "                episode_length = info['episode']['l']\n",
        "\n",
        "                self.episode_rewards.append(episode_reward)\n",
        "                self.episode_lengths.append(episode_length)\n",
        "\n",
        "                if self.verbose:\n",
        "                    print(f\"Episode {len(self.episode_rewards)} ended:\")\n",
        "                    print(f\"  Reward: {episode_reward}\")\n",
        "                    print(f\"  Length: {episode_length}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        total_episodes = len(self.episode_rewards)\n",
        "        if total_episodes == 0:\n",
        "            print(\"No episodes completed. Consider increasing total_timesteps.\")\n",
        "            return\n",
        "\n",
        "        avg_reward = np.mean(self.episode_rewards)\n",
        "        avg_length = np.mean(self.episode_lengths)\n",
        "\n",
        "        print(\"\\n==== Training Summary ====\")\n",
        "        print(f\"Total Episodes: {total_episodes}\")\n",
        "        print(f\"Average Reward per Episode: {avg_reward:.2f}\")\n",
        "        print(f\"Average Episode Length: {avg_length:.2f}\")\n",
        "        print(f\"Final Episode Reward: {self.episode_rewards[-1]:.2f}\")\n",
        "        print(\"==========================\\n\")\n"
      ],
      "metadata": {
        "id": "XW3NanaSJ2nS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===============training DQN==============\n",
        "env = DummyVecEnv([lambda: Monitor(EpilepsyDetectionEnv())])\n",
        "logger_callback = EpilepsyLoggerCallback(verbose=1)\n",
        "\n",
        "model = DQN(\"MlpPolicy\", env, verbose=1,\n",
        "            learning_rate=0.0001,\n",
        "            buffer_size=500000,\n",
        "            learning_starts=10000,\n",
        "            batch_size=32,\n",
        "            gamma=0.98,\n",
        "            exploration_fraction=0.3,\n",
        "            exploration_final_eps=0.05)\n",
        "\n",
        "model.learn(total_timesteps=700000,callback=logger_callback)\n",
        "model.save(\"epilepsy_dqn_model\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaxmLH1bCgzq",
        "outputId": "e4908fd3-a2c8-4ce8-8a0d-e66074388d0a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Episode 2762 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2763 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2764 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2764     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 627      |\n",
            "|    total_timesteps  | 552800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 135699   |\n",
            "----------------------------------\n",
            "Episode 2765 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2766 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2767 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2768 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2768     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 628      |\n",
            "|    total_timesteps  | 553600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.892    |\n",
            "|    n_updates        | 135899   |\n",
            "----------------------------------\n",
            "Episode 2769 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2770 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2771 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2772 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2772     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 629      |\n",
            "|    total_timesteps  | 554400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.77     |\n",
            "|    n_updates        | 136099   |\n",
            "----------------------------------\n",
            "Episode 2773 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2774 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2775 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2776 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2776     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 630      |\n",
            "|    total_timesteps  | 555200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00248  |\n",
            "|    n_updates        | 136299   |\n",
            "----------------------------------\n",
            "Episode 2777 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2778 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2779 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2780 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2780     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 631      |\n",
            "|    total_timesteps  | 556000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.886    |\n",
            "|    n_updates        | 136499   |\n",
            "----------------------------------\n",
            "Episode 2781 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2782 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2783 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2784 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2784     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 632      |\n",
            "|    total_timesteps  | 556800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.59     |\n",
            "|    n_updates        | 136699   |\n",
            "----------------------------------\n",
            "Episode 2785 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2786 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2787 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2788 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2788     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 633      |\n",
            "|    total_timesteps  | 557600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.315    |\n",
            "|    n_updates        | 136899   |\n",
            "----------------------------------\n",
            "Episode 2789 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2790 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2791 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2792 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2792     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 634      |\n",
            "|    total_timesteps  | 558400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.594    |\n",
            "|    n_updates        | 137099   |\n",
            "----------------------------------\n",
            "Episode 2793 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2794 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2795 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2796 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2796     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 635      |\n",
            "|    total_timesteps  | 559200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.595    |\n",
            "|    n_updates        | 137299   |\n",
            "----------------------------------\n",
            "Episode 2797 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2798 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2799 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2800 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2800     |\n",
            "|    fps              | 880      |\n",
            "|    time_elapsed     | 636      |\n",
            "|    total_timesteps  | 560000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.874    |\n",
            "|    n_updates        | 137499   |\n",
            "----------------------------------\n",
            "Episode 2801 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2802 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2803 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2804 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2804     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 637      |\n",
            "|    total_timesteps  | 560800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.592    |\n",
            "|    n_updates        | 137699   |\n",
            "----------------------------------\n",
            "Episode 2805 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2806 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2807 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2808 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2808     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 638      |\n",
            "|    total_timesteps  | 561600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00351  |\n",
            "|    n_updates        | 137899   |\n",
            "----------------------------------\n",
            "Episode 2809 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2810 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2811 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2812 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2812     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 639      |\n",
            "|    total_timesteps  | 562400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.587    |\n",
            "|    n_updates        | 138099   |\n",
            "----------------------------------\n",
            "Episode 2813 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2814 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2815 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2816 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2816     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 640      |\n",
            "|    total_timesteps  | 563200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.985    |\n",
            "|    n_updates        | 138299   |\n",
            "----------------------------------\n",
            "Episode 2817 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2818 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2819 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2820 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2820     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 641      |\n",
            "|    total_timesteps  | 564000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.597    |\n",
            "|    n_updates        | 138499   |\n",
            "----------------------------------\n",
            "Episode 2821 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2822 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2823 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2824 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2824     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 642      |\n",
            "|    total_timesteps  | 564800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00189  |\n",
            "|    n_updates        | 138699   |\n",
            "----------------------------------\n",
            "Episode 2825 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 2826 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2827 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2828 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2828     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 643      |\n",
            "|    total_timesteps  | 565600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.595    |\n",
            "|    n_updates        | 138899   |\n",
            "----------------------------------\n",
            "Episode 2829 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2830 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2831 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2832 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2832     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 644      |\n",
            "|    total_timesteps  | 566400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.388    |\n",
            "|    n_updates        | 139099   |\n",
            "----------------------------------\n",
            "Episode 2833 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2834 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2835 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2836 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2836     |\n",
            "|    fps              | 879      |\n",
            "|    time_elapsed     | 645      |\n",
            "|    total_timesteps  | 567200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00483  |\n",
            "|    n_updates        | 139299   |\n",
            "----------------------------------\n",
            "Episode 2837 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2838 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2839 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2840 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2840     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 646      |\n",
            "|    total_timesteps  | 568000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00299  |\n",
            "|    n_updates        | 139499   |\n",
            "----------------------------------\n",
            "Episode 2841 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2842 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2843 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2844 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2844     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 647      |\n",
            "|    total_timesteps  | 568800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 139699   |\n",
            "----------------------------------\n",
            "Episode 2845 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2846 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2847 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2848 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2848     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 648      |\n",
            "|    total_timesteps  | 569600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.301    |\n",
            "|    n_updates        | 139899   |\n",
            "----------------------------------\n",
            "Episode 2849 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2850 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2851 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2852 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2852     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 649      |\n",
            "|    total_timesteps  | 570400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.438    |\n",
            "|    n_updates        | 140099   |\n",
            "----------------------------------\n",
            "Episode 2853 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2854 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2855 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2856 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2856     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 650      |\n",
            "|    total_timesteps  | 571200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 140299   |\n",
            "----------------------------------\n",
            "Episode 2857 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2858 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2859 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2860 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2860     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 651      |\n",
            "|    total_timesteps  | 572000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.389    |\n",
            "|    n_updates        | 140499   |\n",
            "----------------------------------\n",
            "Episode 2861 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2862 ended:\n",
            "  Reward: -215.0\n",
            "  Length: 200\n",
            "Episode 2863 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2864 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2864     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 652      |\n",
            "|    total_timesteps  | 572800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.892    |\n",
            "|    n_updates        | 140699   |\n",
            "----------------------------------\n",
            "Episode 2865 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2866 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2867 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2868 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2868     |\n",
            "|    fps              | 878      |\n",
            "|    time_elapsed     | 653      |\n",
            "|    total_timesteps  | 573600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 140899   |\n",
            "----------------------------------\n",
            "Episode 2869 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2870 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2871 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 2872 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2872     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 654      |\n",
            "|    total_timesteps  | 574400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00296  |\n",
            "|    n_updates        | 141099   |\n",
            "----------------------------------\n",
            "Episode 2873 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2874 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2875 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 2876 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2876     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 655      |\n",
            "|    total_timesteps  | 575200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.592    |\n",
            "|    n_updates        | 141299   |\n",
            "----------------------------------\n",
            "Episode 2877 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2878 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2879 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2880 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2880     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 656      |\n",
            "|    total_timesteps  | 576000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 141499   |\n",
            "----------------------------------\n",
            "Episode 2881 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2882 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2883 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2884 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2884     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 657      |\n",
            "|    total_timesteps  | 576800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 141699   |\n",
            "----------------------------------\n",
            "Episode 2885 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2886 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2887 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2888 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2888     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 658      |\n",
            "|    total_timesteps  | 577600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.441    |\n",
            "|    n_updates        | 141899   |\n",
            "----------------------------------\n",
            "Episode 2889 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2890 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2891 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2892 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2892     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 659      |\n",
            "|    total_timesteps  | 578400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.48     |\n",
            "|    n_updates        | 142099   |\n",
            "----------------------------------\n",
            "Episode 2893 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2894 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2895 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2896 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2896     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 660      |\n",
            "|    total_timesteps  | 579200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 142299   |\n",
            "----------------------------------\n",
            "Episode 2897 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2898 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2899 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2900 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2900     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 661      |\n",
            "|    total_timesteps  | 580000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.594    |\n",
            "|    n_updates        | 142499   |\n",
            "----------------------------------\n",
            "Episode 2901 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2902 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2903 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2904 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2904     |\n",
            "|    fps              | 877      |\n",
            "|    time_elapsed     | 662      |\n",
            "|    total_timesteps  | 580800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00466  |\n",
            "|    n_updates        | 142699   |\n",
            "----------------------------------\n",
            "Episode 2905 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2906 ended:\n",
            "  Reward: -185.0\n",
            "  Length: 200\n",
            "Episode 2907 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2908 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2908     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 663      |\n",
            "|    total_timesteps  | 581600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.295    |\n",
            "|    n_updates        | 142899   |\n",
            "----------------------------------\n",
            "Episode 2909 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2910 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2911 ended:\n",
            "  Reward: -220.0\n",
            "  Length: 200\n",
            "Episode 2912 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2912     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 664      |\n",
            "|    total_timesteps  | 582400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00174  |\n",
            "|    n_updates        | 143099   |\n",
            "----------------------------------\n",
            "Episode 2913 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2914 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2915 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2916 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2916     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 665      |\n",
            "|    total_timesteps  | 583200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 143299   |\n",
            "----------------------------------\n",
            "Episode 2917 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2918 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2919 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2920 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2920     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 666      |\n",
            "|    total_timesteps  | 584000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.595    |\n",
            "|    n_updates        | 143499   |\n",
            "----------------------------------\n",
            "Episode 2921 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2922 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2923 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2924 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2924     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 667      |\n",
            "|    total_timesteps  | 584800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 143699   |\n",
            "----------------------------------\n",
            "Episode 2925 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2926 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2927 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2928 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2928     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 668      |\n",
            "|    total_timesteps  | 585600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 143899   |\n",
            "----------------------------------\n",
            "Episode 2929 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2930 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2931 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2932 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2932     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 669      |\n",
            "|    total_timesteps  | 586400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0966   |\n",
            "|    n_updates        | 144099   |\n",
            "----------------------------------\n",
            "Episode 2933 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2934 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2935 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2936 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2936     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 670      |\n",
            "|    total_timesteps  | 587200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.884    |\n",
            "|    n_updates        | 144299   |\n",
            "----------------------------------\n",
            "Episode 2937 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2938 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2939 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2940 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2940     |\n",
            "|    fps              | 876      |\n",
            "|    time_elapsed     | 671      |\n",
            "|    total_timesteps  | 588000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 144499   |\n",
            "----------------------------------\n",
            "Episode 2941 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2942 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2943 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2944 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2944     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 672      |\n",
            "|    total_timesteps  | 588800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.889    |\n",
            "|    n_updates        | 144699   |\n",
            "----------------------------------\n",
            "Episode 2945 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2946 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2947 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2948 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2948     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 673      |\n",
            "|    total_timesteps  | 589600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 144899   |\n",
            "----------------------------------\n",
            "Episode 2949 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2950 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2951 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2952 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2952     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 674      |\n",
            "|    total_timesteps  | 590400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.19     |\n",
            "|    n_updates        | 145099   |\n",
            "----------------------------------\n",
            "Episode 2953 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2954 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2955 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2956 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2956     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 675      |\n",
            "|    total_timesteps  | 591200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.592    |\n",
            "|    n_updates        | 145299   |\n",
            "----------------------------------\n",
            "Episode 2957 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2958 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2959 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2960 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2960     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 676      |\n",
            "|    total_timesteps  | 592000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.887    |\n",
            "|    n_updates        | 145499   |\n",
            "----------------------------------\n",
            "Episode 2961 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2962 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2963 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2964 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2964     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 677      |\n",
            "|    total_timesteps  | 592800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.881    |\n",
            "|    n_updates        | 145699   |\n",
            "----------------------------------\n",
            "Episode 2965 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2966 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2967 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2968 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2968     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 678      |\n",
            "|    total_timesteps  | 593600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 145899   |\n",
            "----------------------------------\n",
            "Episode 2969 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2970 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2971 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2972 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2972     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 679      |\n",
            "|    total_timesteps  | 594400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.44     |\n",
            "|    n_updates        | 146099   |\n",
            "----------------------------------\n",
            "Episode 2973 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2974 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2975 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2976 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2976     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 680      |\n",
            "|    total_timesteps  | 595200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.296    |\n",
            "|    n_updates        | 146299   |\n",
            "----------------------------------\n",
            "Episode 2977 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2978 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2979 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2980 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2980     |\n",
            "|    fps              | 875      |\n",
            "|    time_elapsed     | 681      |\n",
            "|    total_timesteps  | 596000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.886    |\n",
            "|    n_updates        | 146499   |\n",
            "----------------------------------\n",
            "Episode 2981 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2982 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2983 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2984 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2984     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 682      |\n",
            "|    total_timesteps  | 596800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00293  |\n",
            "|    n_updates        | 146699   |\n",
            "----------------------------------\n",
            "Episode 2985 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2986 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2987 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 2988 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2988     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 683      |\n",
            "|    total_timesteps  | 597600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.736    |\n",
            "|    n_updates        | 146899   |\n",
            "----------------------------------\n",
            "Episode 2989 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2990 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2991 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2992 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2992     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 684      |\n",
            "|    total_timesteps  | 598400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.296    |\n",
            "|    n_updates        | 147099   |\n",
            "----------------------------------\n",
            "Episode 2993 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2994 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2995 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2996 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 2996     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 685      |\n",
            "|    total_timesteps  | 599200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.987    |\n",
            "|    n_updates        | 147299   |\n",
            "----------------------------------\n",
            "Episode 2997 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2998 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2999 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3000 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3000     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 686      |\n",
            "|    total_timesteps  | 600000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.685    |\n",
            "|    n_updates        | 147499   |\n",
            "----------------------------------\n",
            "Episode 3001 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3002 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3003 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3004 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3004     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 687      |\n",
            "|    total_timesteps  | 600800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 147699   |\n",
            "----------------------------------\n",
            "Episode 3005 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3006 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3007 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3008 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3008     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 688      |\n",
            "|    total_timesteps  | 601600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.887    |\n",
            "|    n_updates        | 147899   |\n",
            "----------------------------------\n",
            "Episode 3009 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3010 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3011 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3012 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3012     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 689      |\n",
            "|    total_timesteps  | 602400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.888    |\n",
            "|    n_updates        | 148099   |\n",
            "----------------------------------\n",
            "Episode 3013 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3014 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3015 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3016 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3016     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 689      |\n",
            "|    total_timesteps  | 603200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.44     |\n",
            "|    n_updates        | 148299   |\n",
            "----------------------------------\n",
            "Episode 3017 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3018 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3019 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3020 ended:\n",
            "  Reward: -220.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3020     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 690      |\n",
            "|    total_timesteps  | 604000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 148499   |\n",
            "----------------------------------\n",
            "Episode 3021 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3022 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 3023 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3024 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3024     |\n",
            "|    fps              | 874      |\n",
            "|    time_elapsed     | 691      |\n",
            "|    total_timesteps  | 604800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00272  |\n",
            "|    n_updates        | 148699   |\n",
            "----------------------------------\n",
            "Episode 3025 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3026 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3027 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3028 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3028     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 692      |\n",
            "|    total_timesteps  | 605600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.885    |\n",
            "|    n_updates        | 148899   |\n",
            "----------------------------------\n",
            "Episode 3029 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3030 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3031 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3032 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3032     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 693      |\n",
            "|    total_timesteps  | 606400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.893    |\n",
            "|    n_updates        | 149099   |\n",
            "----------------------------------\n",
            "Episode 3033 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3034 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3035 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3036 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -129     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3036     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 694      |\n",
            "|    total_timesteps  | 607200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00259  |\n",
            "|    n_updates        | 149299   |\n",
            "----------------------------------\n",
            "Episode 3037 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3038 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3039 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3040 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3040     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 695      |\n",
            "|    total_timesteps  | 608000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 149499   |\n",
            "----------------------------------\n",
            "Episode 3041 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3042 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3043 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3044 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3044     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 696      |\n",
            "|    total_timesteps  | 608800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.887    |\n",
            "|    n_updates        | 149699   |\n",
            "----------------------------------\n",
            "Episode 3045 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3046 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3047 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3048 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3048     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 697      |\n",
            "|    total_timesteps  | 609600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.889    |\n",
            "|    n_updates        | 149899   |\n",
            "----------------------------------\n",
            "Episode 3049 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3050 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3051 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3052 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3052     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 698      |\n",
            "|    total_timesteps  | 610400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.292    |\n",
            "|    n_updates        | 150099   |\n",
            "----------------------------------\n",
            "Episode 3053 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3054 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 3055 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3056 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3056     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 699      |\n",
            "|    total_timesteps  | 611200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.591    |\n",
            "|    n_updates        | 150299   |\n",
            "----------------------------------\n",
            "Episode 3057 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3058 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3059 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 3060 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3060     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 700      |\n",
            "|    total_timesteps  | 612000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.59     |\n",
            "|    n_updates        | 150499   |\n",
            "----------------------------------\n",
            "Episode 3061 ended:\n",
            "  Reward: -40.0\n",
            "  Length: 200\n",
            "Episode 3062 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3063 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3064 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3064     |\n",
            "|    fps              | 873      |\n",
            "|    time_elapsed     | 701      |\n",
            "|    total_timesteps  | 612800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.389    |\n",
            "|    n_updates        | 150699   |\n",
            "----------------------------------\n",
            "Episode 3065 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3066 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3067 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3068 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3068     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 702      |\n",
            "|    total_timesteps  | 613600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 150899   |\n",
            "----------------------------------\n",
            "Episode 3069 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3070 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 3071 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3072 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -128     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3072     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 703      |\n",
            "|    total_timesteps  | 614400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.401    |\n",
            "|    n_updates        | 151099   |\n",
            "----------------------------------\n",
            "Episode 3073 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3074 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3075 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3076 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3076     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 704      |\n",
            "|    total_timesteps  | 615200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 151299   |\n",
            "----------------------------------\n",
            "Episode 3077 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3078 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3079 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3080 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3080     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 705      |\n",
            "|    total_timesteps  | 616000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 151499   |\n",
            "----------------------------------\n",
            "Episode 3081 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3082 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3083 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3084 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3084     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 706      |\n",
            "|    total_timesteps  | 616800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00808  |\n",
            "|    n_updates        | 151699   |\n",
            "----------------------------------\n",
            "Episode 3085 ended:\n",
            "  Reward: -185.0\n",
            "  Length: 200\n",
            "Episode 3086 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3087 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3088 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -126     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3088     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 707      |\n",
            "|    total_timesteps  | 617600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00246  |\n",
            "|    n_updates        | 151899   |\n",
            "----------------------------------\n",
            "Episode 3089 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3090 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3091 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3092 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3092     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 708      |\n",
            "|    total_timesteps  | 618400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.39     |\n",
            "|    n_updates        | 152099   |\n",
            "----------------------------------\n",
            "Episode 3093 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3094 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3095 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3096 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -127     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3096     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 709      |\n",
            "|    total_timesteps  | 619200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 152299   |\n",
            "----------------------------------\n",
            "Episode 3097 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3098 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3099 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3100 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -125     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3100     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 710      |\n",
            "|    total_timesteps  | 620000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.596    |\n",
            "|    n_updates        | 152499   |\n",
            "----------------------------------\n",
            "Episode 3101 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3102 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3103 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 3104 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3104     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 711      |\n",
            "|    total_timesteps  | 620800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.888    |\n",
            "|    n_updates        | 152699   |\n",
            "----------------------------------\n",
            "Episode 3105 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3106 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 3107 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3108 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3108     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 712      |\n",
            "|    total_timesteps  | 621600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 152899   |\n",
            "----------------------------------\n",
            "Episode 3109 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3110 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3111 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3112 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3112     |\n",
            "|    fps              | 872      |\n",
            "|    time_elapsed     | 713      |\n",
            "|    total_timesteps  | 622400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00153  |\n",
            "|    n_updates        | 153099   |\n",
            "----------------------------------\n",
            "Episode 3113 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3114 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3115 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3116 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3116     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 714      |\n",
            "|    total_timesteps  | 623200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 153299   |\n",
            "----------------------------------\n",
            "Episode 3117 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3118 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3119 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3120 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3120     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 715      |\n",
            "|    total_timesteps  | 624000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.594    |\n",
            "|    n_updates        | 153499   |\n",
            "----------------------------------\n",
            "Episode 3121 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3122 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3123 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3124 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3124     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 716      |\n",
            "|    total_timesteps  | 624800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.884    |\n",
            "|    n_updates        | 153699   |\n",
            "----------------------------------\n",
            "Episode 3125 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3126 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3127 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3128 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3128     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 717      |\n",
            "|    total_timesteps  | 625600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 153899   |\n",
            "----------------------------------\n",
            "Episode 3129 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3130 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3131 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3132 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3132     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 718      |\n",
            "|    total_timesteps  | 626400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00162  |\n",
            "|    n_updates        | 154099   |\n",
            "----------------------------------\n",
            "Episode 3133 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3134 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3135 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3136 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3136     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 719      |\n",
            "|    total_timesteps  | 627200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.438    |\n",
            "|    n_updates        | 154299   |\n",
            "----------------------------------\n",
            "Episode 3137 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3138 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3139 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3140 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3140     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 720      |\n",
            "|    total_timesteps  | 628000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.283    |\n",
            "|    n_updates        | 154499   |\n",
            "----------------------------------\n",
            "Episode 3141 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3142 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3143 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3144 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3144     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 721      |\n",
            "|    total_timesteps  | 628800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.598    |\n",
            "|    n_updates        | 154699   |\n",
            "----------------------------------\n",
            "Episode 3145 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3146 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3147 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3148 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3148     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 722      |\n",
            "|    total_timesteps  | 629600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.581    |\n",
            "|    n_updates        | 154899   |\n",
            "----------------------------------\n",
            "Episode 3149 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3150 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3151 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3152 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3152     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 723      |\n",
            "|    total_timesteps  | 630400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00243  |\n",
            "|    n_updates        | 155099   |\n",
            "----------------------------------\n",
            "Episode 3153 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3154 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3155 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3156 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3156     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 724      |\n",
            "|    total_timesteps  | 631200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.985    |\n",
            "|    n_updates        | 155299   |\n",
            "----------------------------------\n",
            "Episode 3157 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3158 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3159 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3160 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3160     |\n",
            "|    fps              | 871      |\n",
            "|    time_elapsed     | 725      |\n",
            "|    total_timesteps  | 632000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.89     |\n",
            "|    n_updates        | 155499   |\n",
            "----------------------------------\n",
            "Episode 3161 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3162 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3163 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3164 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3164     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 726      |\n",
            "|    total_timesteps  | 632800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 155699   |\n",
            "----------------------------------\n",
            "Episode 3165 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3166 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3167 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3168 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3168     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 727      |\n",
            "|    total_timesteps  | 633600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 155899   |\n",
            "----------------------------------\n",
            "Episode 3169 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3170 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3171 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3172 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3172     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 728      |\n",
            "|    total_timesteps  | 634400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 156099   |\n",
            "----------------------------------\n",
            "Episode 3173 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3174 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3175 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3176 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3176     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 729      |\n",
            "|    total_timesteps  | 635200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00296  |\n",
            "|    n_updates        | 156299   |\n",
            "----------------------------------\n",
            "Episode 3177 ended:\n",
            "  Reward: -40.0\n",
            "  Length: 200\n",
            "Episode 3178 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3179 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3180 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3180     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 730      |\n",
            "|    total_timesteps  | 636000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00236  |\n",
            "|    n_updates        | 156499   |\n",
            "----------------------------------\n",
            "Episode 3181 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3182 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3183 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3184 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3184     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 731      |\n",
            "|    total_timesteps  | 636800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00287  |\n",
            "|    n_updates        | 156699   |\n",
            "----------------------------------\n",
            "Episode 3185 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3186 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3187 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3188 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3188     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 732      |\n",
            "|    total_timesteps  | 637600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.592    |\n",
            "|    n_updates        | 156899   |\n",
            "----------------------------------\n",
            "Episode 3189 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3190 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3191 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3192 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3192     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 733      |\n",
            "|    total_timesteps  | 638400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.884    |\n",
            "|    n_updates        | 157099   |\n",
            "----------------------------------\n",
            "Episode 3193 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3194 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3195 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3196 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3196     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 734      |\n",
            "|    total_timesteps  | 639200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.404    |\n",
            "|    n_updates        | 157299   |\n",
            "----------------------------------\n",
            "Episode 3197 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3198 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3199 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3200 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3200     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 735      |\n",
            "|    total_timesteps  | 640000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.596    |\n",
            "|    n_updates        | 157499   |\n",
            "----------------------------------\n",
            "Episode 3201 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3202 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3203 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3204 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3204     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 736      |\n",
            "|    total_timesteps  | 640800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.887    |\n",
            "|    n_updates        | 157699   |\n",
            "----------------------------------\n",
            "Episode 3205 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3206 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3207 ended:\n",
            "  Reward: -185.0\n",
            "  Length: 200\n",
            "Episode 3208 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3208     |\n",
            "|    fps              | 870      |\n",
            "|    time_elapsed     | 737      |\n",
            "|    total_timesteps  | 641600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 157899   |\n",
            "----------------------------------\n",
            "Episode 3209 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3210 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3211 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3212 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3212     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 738      |\n",
            "|    total_timesteps  | 642400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.19     |\n",
            "|    n_updates        | 158099   |\n",
            "----------------------------------\n",
            "Episode 3213 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3214 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3215 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3216 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3216     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 739      |\n",
            "|    total_timesteps  | 643200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 158299   |\n",
            "----------------------------------\n",
            "Episode 3217 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3218 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3219 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3220 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3220     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 740      |\n",
            "|    total_timesteps  | 644000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.993    |\n",
            "|    n_updates        | 158499   |\n",
            "----------------------------------\n",
            "Episode 3221 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3222 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3223 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3224 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3224     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 741      |\n",
            "|    total_timesteps  | 644800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.592    |\n",
            "|    n_updates        | 158699   |\n",
            "----------------------------------\n",
            "Episode 3225 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3226 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3227 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 3228 ended:\n",
            "  Reward: -225.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3228     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 742      |\n",
            "|    total_timesteps  | 645600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.694    |\n",
            "|    n_updates        | 158899   |\n",
            "----------------------------------\n",
            "Episode 3229 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3230 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3231 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3232 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3232     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 743      |\n",
            "|    total_timesteps  | 646400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.588    |\n",
            "|    n_updates        | 159099   |\n",
            "----------------------------------\n",
            "Episode 3233 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3234 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3235 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3236 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3236     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 744      |\n",
            "|    total_timesteps  | 647200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 159299   |\n",
            "----------------------------------\n",
            "Episode 3237 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3238 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 3239 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3240 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3240     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 745      |\n",
            "|    total_timesteps  | 648000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.407    |\n",
            "|    n_updates        | 159499   |\n",
            "----------------------------------\n",
            "Episode 3241 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3242 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3243 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3244 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3244     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 746      |\n",
            "|    total_timesteps  | 648800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 159699   |\n",
            "----------------------------------\n",
            "Episode 3245 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3246 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3247 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3248 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3248     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 747      |\n",
            "|    total_timesteps  | 649600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.41     |\n",
            "|    n_updates        | 159899   |\n",
            "----------------------------------\n",
            "Episode 3249 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3250 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3251 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3252 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3252     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 748      |\n",
            "|    total_timesteps  | 650400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.888    |\n",
            "|    n_updates        | 160099   |\n",
            "----------------------------------\n",
            "Episode 3253 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3254 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3255 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3256 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -124     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3256     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 749      |\n",
            "|    total_timesteps  | 651200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 160299   |\n",
            "----------------------------------\n",
            "Episode 3257 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3258 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3259 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3260 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3260     |\n",
            "|    fps              | 869      |\n",
            "|    time_elapsed     | 750      |\n",
            "|    total_timesteps  | 652000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 160499   |\n",
            "----------------------------------\n",
            "Episode 3261 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3262 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3263 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3264 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3264     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 751      |\n",
            "|    total_timesteps  | 652800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.296    |\n",
            "|    n_updates        | 160699   |\n",
            "----------------------------------\n",
            "Episode 3265 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3266 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3267 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3268 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -123     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3268     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 752      |\n",
            "|    total_timesteps  | 653600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.005    |\n",
            "|    n_updates        | 160899   |\n",
            "----------------------------------\n",
            "Episode 3269 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3270 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3271 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3272 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3272     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 753      |\n",
            "|    total_timesteps  | 654400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.399    |\n",
            "|    n_updates        | 161099   |\n",
            "----------------------------------\n",
            "Episode 3273 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3274 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3275 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3276 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3276     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 754      |\n",
            "|    total_timesteps  | 655200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.594    |\n",
            "|    n_updates        | 161299   |\n",
            "----------------------------------\n",
            "Episode 3277 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3278 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3279 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3280 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3280     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 755      |\n",
            "|    total_timesteps  | 656000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00195  |\n",
            "|    n_updates        | 161499   |\n",
            "----------------------------------\n",
            "Episode 3281 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3282 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3283 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3284 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -122     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3284     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 756      |\n",
            "|    total_timesteps  | 656800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.693    |\n",
            "|    n_updates        | 161699   |\n",
            "----------------------------------\n",
            "Episode 3285 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3286 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3287 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3288 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3288     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 757      |\n",
            "|    total_timesteps  | 657600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.599    |\n",
            "|    n_updates        | 161899   |\n",
            "----------------------------------\n",
            "Episode 3289 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3290 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3291 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3292 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3292     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 758      |\n",
            "|    total_timesteps  | 658400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 162099   |\n",
            "----------------------------------\n",
            "Episode 3293 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3294 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3295 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3296 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3296     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 759      |\n",
            "|    total_timesteps  | 659200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.894    |\n",
            "|    n_updates        | 162299   |\n",
            "----------------------------------\n",
            "Episode 3297 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3298 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3299 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3300 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3300     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 760      |\n",
            "|    total_timesteps  | 660000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.888    |\n",
            "|    n_updates        | 162499   |\n",
            "----------------------------------\n",
            "Episode 3301 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3302 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3303 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3304 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3304     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 761      |\n",
            "|    total_timesteps  | 660800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 162699   |\n",
            "----------------------------------\n",
            "Episode 3305 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3306 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3307 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3308 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3308     |\n",
            "|    fps              | 868      |\n",
            "|    time_elapsed     | 762      |\n",
            "|    total_timesteps  | 661600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 162899   |\n",
            "----------------------------------\n",
            "Episode 3309 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3310 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3311 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3312 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3312     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 763      |\n",
            "|    total_timesteps  | 662400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 163099   |\n",
            "----------------------------------\n",
            "Episode 3313 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3314 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3315 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3316 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3316     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 764      |\n",
            "|    total_timesteps  | 663200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 163299   |\n",
            "----------------------------------\n",
            "Episode 3317 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3318 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3319 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3320 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3320     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 765      |\n",
            "|    total_timesteps  | 664000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.303    |\n",
            "|    n_updates        | 163499   |\n",
            "----------------------------------\n",
            "Episode 3321 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3322 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3323 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3324 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3324     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 766      |\n",
            "|    total_timesteps  | 664800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.435    |\n",
            "|    n_updates        | 163699   |\n",
            "----------------------------------\n",
            "Episode 3325 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3326 ended:\n",
            "  Reward: -185.0\n",
            "  Length: 200\n",
            "Episode 3327 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3328 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3328     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 767      |\n",
            "|    total_timesteps  | 665600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.589    |\n",
            "|    n_updates        | 163899   |\n",
            "----------------------------------\n",
            "Episode 3329 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3330 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3331 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3332 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3332     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 768      |\n",
            "|    total_timesteps  | 666400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 164099   |\n",
            "----------------------------------\n",
            "Episode 3333 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3334 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3335 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3336 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3336     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 769      |\n",
            "|    total_timesteps  | 667200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.593    |\n",
            "|    n_updates        | 164299   |\n",
            "----------------------------------\n",
            "Episode 3337 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3338 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3339 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3340 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3340     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 770      |\n",
            "|    total_timesteps  | 668000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00645  |\n",
            "|    n_updates        | 164499   |\n",
            "----------------------------------\n",
            "Episode 3341 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3342 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3343 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3344 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3344     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 771      |\n",
            "|    total_timesteps  | 668800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.884    |\n",
            "|    n_updates        | 164699   |\n",
            "----------------------------------\n",
            "Episode 3345 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3346 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3347 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3348 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3348     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 772      |\n",
            "|    total_timesteps  | 669600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.89     |\n",
            "|    n_updates        | 164899   |\n",
            "----------------------------------\n",
            "Episode 3349 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3350 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3351 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3352 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3352     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 773      |\n",
            "|    total_timesteps  | 670400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00184  |\n",
            "|    n_updates        | 165099   |\n",
            "----------------------------------\n",
            "Episode 3353 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3354 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3355 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3356 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3356     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 774      |\n",
            "|    total_timesteps  | 671200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 165299   |\n",
            "----------------------------------\n",
            "Episode 3357 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3358 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3359 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3360 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3360     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 775      |\n",
            "|    total_timesteps  | 672000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.434    |\n",
            "|    n_updates        | 165499   |\n",
            "----------------------------------\n",
            "Episode 3361 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3362 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3363 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3364 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3364     |\n",
            "|    fps              | 867      |\n",
            "|    time_elapsed     | 775      |\n",
            "|    total_timesteps  | 672800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.409    |\n",
            "|    n_updates        | 165699   |\n",
            "----------------------------------\n",
            "Episode 3365 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3366 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3367 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3368 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3368     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 776      |\n",
            "|    total_timesteps  | 673600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.596    |\n",
            "|    n_updates        | 165899   |\n",
            "----------------------------------\n",
            "Episode 3369 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3370 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3371 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3372 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3372     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 777      |\n",
            "|    total_timesteps  | 674400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 166099   |\n",
            "----------------------------------\n",
            "Episode 3373 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3374 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3375 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3376 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3376     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 778      |\n",
            "|    total_timesteps  | 675200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0026   |\n",
            "|    n_updates        | 166299   |\n",
            "----------------------------------\n",
            "Episode 3377 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3378 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3379 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3380 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3380     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 779      |\n",
            "|    total_timesteps  | 676000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00295  |\n",
            "|    n_updates        | 166499   |\n",
            "----------------------------------\n",
            "Episode 3381 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3382 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3383 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3384 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3384     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 780      |\n",
            "|    total_timesteps  | 676800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.703    |\n",
            "|    n_updates        | 166699   |\n",
            "----------------------------------\n",
            "Episode 3385 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3386 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3387 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3388 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3388     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 781      |\n",
            "|    total_timesteps  | 677600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 166899   |\n",
            "----------------------------------\n",
            "Episode 3389 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3390 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3391 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3392 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3392     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 782      |\n",
            "|    total_timesteps  | 678400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.597    |\n",
            "|    n_updates        | 167099   |\n",
            "----------------------------------\n",
            "Episode 3393 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3394 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3395 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3396 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3396     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 783      |\n",
            "|    total_timesteps  | 679200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.889    |\n",
            "|    n_updates        | 167299   |\n",
            "----------------------------------\n",
            "Episode 3397 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3398 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3399 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3400 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3400     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 784      |\n",
            "|    total_timesteps  | 680000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 167499   |\n",
            "----------------------------------\n",
            "Episode 3401 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3402 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3403 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3404 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3404     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 785      |\n",
            "|    total_timesteps  | 680800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.887    |\n",
            "|    n_updates        | 167699   |\n",
            "----------------------------------\n",
            "Episode 3405 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3406 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3407 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3408 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3408     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 786      |\n",
            "|    total_timesteps  | 681600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00421  |\n",
            "|    n_updates        | 167899   |\n",
            "----------------------------------\n",
            "Episode 3409 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3410 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3411 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3412 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3412     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 787      |\n",
            "|    total_timesteps  | 682400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 168099   |\n",
            "----------------------------------\n",
            "Episode 3413 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3414 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3415 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3416 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3416     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 788      |\n",
            "|    total_timesteps  | 683200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.913    |\n",
            "|    n_updates        | 168299   |\n",
            "----------------------------------\n",
            "Episode 3417 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3418 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 3419 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3420 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3420     |\n",
            "|    fps              | 866      |\n",
            "|    time_elapsed     | 789      |\n",
            "|    total_timesteps  | 684000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.707    |\n",
            "|    n_updates        | 168499   |\n",
            "----------------------------------\n",
            "Episode 3421 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3422 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3423 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3424 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3424     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 790      |\n",
            "|    total_timesteps  | 684800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.295    |\n",
            "|    n_updates        | 168699   |\n",
            "----------------------------------\n",
            "Episode 3425 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3426 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3427 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3428 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3428     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 791      |\n",
            "|    total_timesteps  | 685600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.19     |\n",
            "|    n_updates        | 168899   |\n",
            "----------------------------------\n",
            "Episode 3429 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3430 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3431 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3432 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3432     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 792      |\n",
            "|    total_timesteps  | 686400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 169099   |\n",
            "----------------------------------\n",
            "Episode 3433 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3434 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3435 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3436 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3436     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 793      |\n",
            "|    total_timesteps  | 687200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.03     |\n",
            "|    n_updates        | 169299   |\n",
            "----------------------------------\n",
            "Episode 3437 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3438 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3439 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3440 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3440     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 794      |\n",
            "|    total_timesteps  | 688000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.59     |\n",
            "|    n_updates        | 169499   |\n",
            "----------------------------------\n",
            "Episode 3441 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3442 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3443 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3444 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3444     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 795      |\n",
            "|    total_timesteps  | 688800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.299    |\n",
            "|    n_updates        | 169699   |\n",
            "----------------------------------\n",
            "Episode 3445 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3446 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3447 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3448 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3448     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 796      |\n",
            "|    total_timesteps  | 689600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.995    |\n",
            "|    n_updates        | 169899   |\n",
            "----------------------------------\n",
            "Episode 3449 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3450 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3451 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3452 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3452     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 797      |\n",
            "|    total_timesteps  | 690400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00455  |\n",
            "|    n_updates        | 170099   |\n",
            "----------------------------------\n",
            "Episode 3453 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3454 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3455 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3456 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3456     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 798      |\n",
            "|    total_timesteps  | 691200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00337  |\n",
            "|    n_updates        | 170299   |\n",
            "----------------------------------\n",
            "Episode 3457 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3458 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3459 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3460 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -115     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3460     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 799      |\n",
            "|    total_timesteps  | 692000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.73     |\n",
            "|    n_updates        | 170499   |\n",
            "----------------------------------\n",
            "Episode 3461 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3462 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3463 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3464 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3464     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 800      |\n",
            "|    total_timesteps  | 692800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.3      |\n",
            "|    n_updates        | 170699   |\n",
            "----------------------------------\n",
            "Episode 3465 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3466 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3467 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3468 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3468     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 801      |\n",
            "|    total_timesteps  | 693600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 170899   |\n",
            "----------------------------------\n",
            "Episode 3469 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3470 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3471 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3472 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -116     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3472     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 802      |\n",
            "|    total_timesteps  | 694400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.893    |\n",
            "|    n_updates        | 171099   |\n",
            "----------------------------------\n",
            "Episode 3473 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3474 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3475 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3476 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3476     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 803      |\n",
            "|    total_timesteps  | 695200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.18     |\n",
            "|    n_updates        | 171299   |\n",
            "----------------------------------\n",
            "Episode 3477 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3478 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3479 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3480 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -117     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3480     |\n",
            "|    fps              | 865      |\n",
            "|    time_elapsed     | 804      |\n",
            "|    total_timesteps  | 696000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.297    |\n",
            "|    n_updates        | 171499   |\n",
            "----------------------------------\n",
            "Episode 3481 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3482 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3483 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3484 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -118     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3484     |\n",
            "|    fps              | 864      |\n",
            "|    time_elapsed     | 805      |\n",
            "|    total_timesteps  | 696800   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.298    |\n",
            "|    n_updates        | 171699   |\n",
            "----------------------------------\n",
            "Episode 3485 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3486 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3487 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3488 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -119     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3488     |\n",
            "|    fps              | 864      |\n",
            "|    time_elapsed     | 806      |\n",
            "|    total_timesteps  | 697600   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00422  |\n",
            "|    n_updates        | 171899   |\n",
            "----------------------------------\n",
            "Episode 3489 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3490 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3491 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3492 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3492     |\n",
            "|    fps              | 864      |\n",
            "|    time_elapsed     | 807      |\n",
            "|    total_timesteps  | 698400   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.734    |\n",
            "|    n_updates        | 172099   |\n",
            "----------------------------------\n",
            "Episode 3493 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3494 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3495 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3496 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -120     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3496     |\n",
            "|    fps              | 864      |\n",
            "|    time_elapsed     | 808      |\n",
            "|    total_timesteps  | 699200   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 1.32     |\n",
            "|    n_updates        | 172299   |\n",
            "----------------------------------\n",
            "Episode 3497 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3498 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3499 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3500 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 200      |\n",
            "|    ep_rew_mean      | -121     |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 3500     |\n",
            "|    fps              | 864      |\n",
            "|    time_elapsed     | 809      |\n",
            "|    total_timesteps  | 700000   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.00253  |\n",
            "|    n_updates        | 172499   |\n",
            "----------------------------------\n",
            "\n",
            "==== Training Summary ====\n",
            "Total Episodes: 3500\n",
            "Average Reward per Episode: -122.33\n",
            "Average Episode Length: 200.00\n",
            "Final Episode Reward: -155.00\n",
            "==========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#=========training PPO========\n",
        "env = DummyVecEnv([lambda: Monitor(EpilepsyDetectionEnv())])\n",
        "logger_callback = EpilepsyLoggerCallback(verbose=1)\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1,\n",
        "            learning_rate=0.0001,\n",
        "            gamma=0.98,\n",
        "            batch_size=16,\n",
        "            n_steps=4096)\n",
        "\n",
        "model.learn(total_timesteps=700000,callback=logger_callback)\n",
        "model.save(\"epilepsy_ppo_model\")\n",
        "\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuiwHPtDHxzY",
        "outputId": "f2ab22ff-0259-4332-8ba1-0b4365d9ce1a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Episode 2266 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2267 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2268 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2269 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2270 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2271 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2272 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2273 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -124         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 111          |\n",
            "|    time_elapsed         | 1901         |\n",
            "|    total_timesteps      | 454656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046553286 |\n",
            "|    clip_fraction        | 0.0393       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.997       |\n",
            "|    explained_variance   | 0.000845     |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 27.6         |\n",
            "|    n_updates            | 1100         |\n",
            "|    policy_gradient_loss | -0.00396     |\n",
            "|    value_loss           | 55.8         |\n",
            "------------------------------------------\n",
            "Episode 2274 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2275 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2276 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2277 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2278 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2279 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2280 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2281 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2282 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2283 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2284 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2285 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2286 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2287 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2288 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2289 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2290 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2291 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2292 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2293 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -126        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 112         |\n",
            "|    time_elapsed         | 1918        |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005199822 |\n",
            "|    clip_fraction        | 0.0523      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | -0.0286     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 12          |\n",
            "|    n_updates            | 1110        |\n",
            "|    policy_gradient_loss | -0.00481    |\n",
            "|    value_loss           | 60          |\n",
            "-----------------------------------------\n",
            "Episode 2294 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 2295 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2296 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2297 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2298 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 2299 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2300 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2301 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2302 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 2303 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2304 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2305 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2306 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2307 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2308 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2309 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2310 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2311 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2312 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2313 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2314 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -122        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 113         |\n",
            "|    time_elapsed         | 1935        |\n",
            "|    total_timesteps      | 462848      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007002513 |\n",
            "|    clip_fraction        | 0.0669      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.16       |\n",
            "|    explained_variance   | 0.000433    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 31          |\n",
            "|    n_updates            | 1120        |\n",
            "|    policy_gradient_loss | -0.00563    |\n",
            "|    value_loss           | 58.5        |\n",
            "-----------------------------------------\n",
            "Episode 2315 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2316 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2317 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2318 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2319 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2320 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2321 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2322 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2323 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2324 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2325 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2326 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2327 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2328 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2329 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2330 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2331 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2332 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 2333 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2334 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -127         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 114          |\n",
            "|    time_elapsed         | 1953         |\n",
            "|    total_timesteps      | 466944       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043758303 |\n",
            "|    clip_fraction        | 0.0396       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.13        |\n",
            "|    explained_variance   | 0.00231      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 33.1         |\n",
            "|    n_updates            | 1130         |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    value_loss           | 55           |\n",
            "------------------------------------------\n",
            "Episode 2335 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2336 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2337 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2338 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2339 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 2340 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2341 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2342 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2343 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2344 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2345 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2346 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2347 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2348 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2349 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2350 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2351 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2352 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2353 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2354 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2355 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -124        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 115         |\n",
            "|    time_elapsed         | 1970        |\n",
            "|    total_timesteps      | 471040      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005339319 |\n",
            "|    clip_fraction        | 0.0573      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.07       |\n",
            "|    explained_variance   | -0.0178     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 21.5        |\n",
            "|    n_updates            | 1140        |\n",
            "|    policy_gradient_loss | -0.00404    |\n",
            "|    value_loss           | 50          |\n",
            "-----------------------------------------\n",
            "Episode 2356 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2357 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2358 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2359 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 2360 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2361 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2362 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2363 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2364 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2365 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2366 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2367 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2368 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2369 ended:\n",
            "  Reward: -185.0\n",
            "  Length: 200\n",
            "Episode 2370 ended:\n",
            "  Reward: -40.0\n",
            "  Length: 200\n",
            "Episode 2371 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2372 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2373 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2374 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2375 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -124        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 1987        |\n",
            "|    total_timesteps      | 475136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004298087 |\n",
            "|    clip_fraction        | 0.0317      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.00214     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 25.7        |\n",
            "|    n_updates            | 1150        |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    value_loss           | 53          |\n",
            "-----------------------------------------\n",
            "Episode 2376 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2377 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2378 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2379 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2380 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2381 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2382 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2383 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2384 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2385 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2386 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2387 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2388 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2389 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2390 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2391 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2392 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2393 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2394 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2395 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2396 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 117          |\n",
            "|    time_elapsed         | 2004         |\n",
            "|    total_timesteps      | 479232       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041495664 |\n",
            "|    clip_fraction        | 0.0417       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.07        |\n",
            "|    explained_variance   | -0.0222      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 16.3         |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | -0.00354     |\n",
            "|    value_loss           | 59.6         |\n",
            "------------------------------------------\n",
            "Episode 2397 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2398 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2399 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2400 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2401 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2402 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2403 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2404 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2405 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2406 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2407 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2408 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2409 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2410 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2411 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2412 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2413 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2414 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2415 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2416 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -122         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 118          |\n",
            "|    time_elapsed         | 2022         |\n",
            "|    total_timesteps      | 483328       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054781684 |\n",
            "|    clip_fraction        | 0.0543       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.966       |\n",
            "|    explained_variance   | 0.0746       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 27.6         |\n",
            "|    n_updates            | 1170         |\n",
            "|    policy_gradient_loss | -0.00491     |\n",
            "|    value_loss           | 46.3         |\n",
            "------------------------------------------\n",
            "Episode 2417 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 2418 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2419 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 2420 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2421 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2422 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2423 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2424 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2425 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2426 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2427 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 2428 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2429 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2430 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2431 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2432 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2433 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2434 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2435 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2436 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2437 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -122        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 119         |\n",
            "|    time_elapsed         | 2039        |\n",
            "|    total_timesteps      | 487424      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005247916 |\n",
            "|    clip_fraction        | 0.0405      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | -0.0296     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 10.7        |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | -0.00341    |\n",
            "|    value_loss           | 53.9        |\n",
            "-----------------------------------------\n",
            "Episode 2438 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2439 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2440 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2441 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2442 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2443 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2444 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2445 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2446 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2447 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2448 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2449 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2450 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2451 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2452 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2453 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2454 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2455 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2456 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2457 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 120          |\n",
            "|    time_elapsed         | 2056         |\n",
            "|    total_timesteps      | 491520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039729197 |\n",
            "|    clip_fraction        | 0.0321       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.875       |\n",
            "|    explained_variance   | -0.0437      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 21.3         |\n",
            "|    n_updates            | 1190         |\n",
            "|    policy_gradient_loss | -0.00283     |\n",
            "|    value_loss           | 52.2         |\n",
            "------------------------------------------\n",
            "Episode 2458 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2459 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2460 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2461 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2462 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2463 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2464 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2465 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2466 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2467 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2468 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2469 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 2470 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2471 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2472 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2473 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2474 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2475 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 2476 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2477 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2478 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 121          |\n",
            "|    time_elapsed         | 2073         |\n",
            "|    total_timesteps      | 495616       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045958357 |\n",
            "|    clip_fraction        | 0.055        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.973       |\n",
            "|    explained_variance   | -0.0151      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 29.4         |\n",
            "|    n_updates            | 1200         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 52.6         |\n",
            "------------------------------------------\n",
            "Episode 2479 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2480 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2481 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2482 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2483 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2484 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2485 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2486 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2487 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2488 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2489 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2490 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2491 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2492 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2493 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2494 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2495 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2496 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2497 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2498 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -119        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 238         |\n",
            "|    iterations           | 122         |\n",
            "|    time_elapsed         | 2090        |\n",
            "|    total_timesteps      | 499712      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005758062 |\n",
            "|    clip_fraction        | 0.0567      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.00871     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 23.8        |\n",
            "|    n_updates            | 1210        |\n",
            "|    policy_gradient_loss | -0.00399    |\n",
            "|    value_loss           | 50.1        |\n",
            "-----------------------------------------\n",
            "Episode 2499 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2500 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2501 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2502 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2503 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2504 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2505 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2506 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2507 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2508 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2509 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2510 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2511 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2512 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2513 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2514 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2515 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2516 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2517 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2518 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2519 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -117        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 123         |\n",
            "|    time_elapsed         | 2107        |\n",
            "|    total_timesteps      | 503808      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004669861 |\n",
            "|    clip_fraction        | 0.0528      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.01       |\n",
            "|    explained_variance   | 0.0729      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 12.3        |\n",
            "|    n_updates            | 1220        |\n",
            "|    policy_gradient_loss | -0.00535    |\n",
            "|    value_loss           | 50.3        |\n",
            "-----------------------------------------\n",
            "Episode 2520 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2521 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2522 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2523 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2524 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2525 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2526 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2527 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2528 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2529 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2530 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2531 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2532 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2533 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2534 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2535 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2536 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2537 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2538 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 2539 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -114         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 2125         |\n",
            "|    total_timesteps      | 507904       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048401356 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.977       |\n",
            "|    explained_variance   | -0.0615      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 17.4         |\n",
            "|    n_updates            | 1230         |\n",
            "|    policy_gradient_loss | -0.00317     |\n",
            "|    value_loss           | 44.3         |\n",
            "------------------------------------------\n",
            "Episode 2540 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2541 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2542 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2543 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2544 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2545 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2546 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2547 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2548 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2549 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2550 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2551 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2552 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2553 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2554 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2555 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2556 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2557 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2558 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2559 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2560 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -114         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 2142         |\n",
            "|    total_timesteps      | 512000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0046078726 |\n",
            "|    clip_fraction        | 0.0574       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.835       |\n",
            "|    explained_variance   | -0.0171      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 30.8         |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.00514     |\n",
            "|    value_loss           | 49.1         |\n",
            "------------------------------------------\n",
            "Episode 2561 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2562 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2563 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2564 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2565 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2566 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2567 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2568 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2569 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2570 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2571 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2572 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2573 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2574 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2575 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2576 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2577 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2578 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2579 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2580 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -114         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 126          |\n",
            "|    time_elapsed         | 2159         |\n",
            "|    total_timesteps      | 516096       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045304983 |\n",
            "|    clip_fraction        | 0.0409       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.01        |\n",
            "|    explained_variance   | 0.00621      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 28           |\n",
            "|    n_updates            | 1250         |\n",
            "|    policy_gradient_loss | -0.00379     |\n",
            "|    value_loss           | 51.3         |\n",
            "------------------------------------------\n",
            "Episode 2581 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2582 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2583 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2584 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2585 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2586 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2587 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2588 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2589 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2590 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2591 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2592 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2593 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2594 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2595 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2596 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2597 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 2598 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 2599 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2600 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -116        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 127         |\n",
            "|    time_elapsed         | 2176        |\n",
            "|    total_timesteps      | 520192      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005087255 |\n",
            "|    clip_fraction        | 0.0439      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.983      |\n",
            "|    explained_variance   | -0.0541     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 16.8        |\n",
            "|    n_updates            | 1260        |\n",
            "|    policy_gradient_loss | -0.00327    |\n",
            "|    value_loss           | 50.4        |\n",
            "-----------------------------------------\n",
            "Episode 2601 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2602 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2603 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2604 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2605 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2606 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2607 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2608 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2609 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2610 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2611 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2612 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2613 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2614 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2615 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2616 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2617 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2618 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2619 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2620 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2621 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -117        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 128         |\n",
            "|    time_elapsed         | 2193        |\n",
            "|    total_timesteps      | 524288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005551357 |\n",
            "|    clip_fraction        | 0.0508      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.989      |\n",
            "|    explained_variance   | 0.00973     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 24.5        |\n",
            "|    n_updates            | 1270        |\n",
            "|    policy_gradient_loss | -0.00479    |\n",
            "|    value_loss           | 55          |\n",
            "-----------------------------------------\n",
            "Episode 2622 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2623 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2624 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2625 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2626 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2627 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2628 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2629 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2630 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2631 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2632 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2633 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2634 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2635 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2636 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2637 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2638 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2639 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2640 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2641 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 2210         |\n",
            "|    total_timesteps      | 528384       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063336627 |\n",
            "|    clip_fraction        | 0.0535       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0.00721      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 23.1         |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.00479     |\n",
            "|    value_loss           | 47.3         |\n",
            "------------------------------------------\n",
            "Episode 2642 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2643 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2644 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2645 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2646 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2647 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2648 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2649 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2650 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2651 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2652 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2653 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2654 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2655 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2656 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2657 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2658 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 2659 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2660 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2661 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2662 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -119         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 130          |\n",
            "|    time_elapsed         | 2228         |\n",
            "|    total_timesteps      | 532480       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043945475 |\n",
            "|    clip_fraction        | 0.0588       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.857       |\n",
            "|    explained_variance   | 0.0358       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 16.2         |\n",
            "|    n_updates            | 1290         |\n",
            "|    policy_gradient_loss | -0.00443     |\n",
            "|    value_loss           | 50           |\n",
            "------------------------------------------\n",
            "Episode 2663 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2664 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2665 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2666 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2667 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2668 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 2669 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2670 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2671 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2672 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2673 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2674 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2675 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2676 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2677 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2678 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2679 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2680 ended:\n",
            "  Reward: -195.0\n",
            "  Length: 200\n",
            "Episode 2681 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2682 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -120        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 238         |\n",
            "|    iterations           | 131         |\n",
            "|    time_elapsed         | 2245        |\n",
            "|    total_timesteps      | 536576      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003273395 |\n",
            "|    clip_fraction        | 0.0447      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.949      |\n",
            "|    explained_variance   | -0.0204     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 36.9        |\n",
            "|    n_updates            | 1300        |\n",
            "|    policy_gradient_loss | -0.00367    |\n",
            "|    value_loss           | 45.4        |\n",
            "-----------------------------------------\n",
            "Episode 2683 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2684 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2685 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2686 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2687 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2688 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2689 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2690 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2691 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2692 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2693 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2694 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2695 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2696 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2697 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2698 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2699 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2700 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2701 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2702 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2703 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 2262         |\n",
            "|    total_timesteps      | 540672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054581077 |\n",
            "|    clip_fraction        | 0.0446       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.915       |\n",
            "|    explained_variance   | -0.0189      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 17.6         |\n",
            "|    n_updates            | 1310         |\n",
            "|    policy_gradient_loss | -0.00398     |\n",
            "|    value_loss           | 46.3         |\n",
            "------------------------------------------\n",
            "Episode 2704 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2705 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2706 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2707 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2708 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2709 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2710 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2711 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2712 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2713 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2714 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2715 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2716 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2717 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2718 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2719 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2720 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2721 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2722 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2723 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 2279         |\n",
            "|    total_timesteps      | 544768       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0043622972 |\n",
            "|    clip_fraction        | 0.0374       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0.0913       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 13           |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    value_loss           | 52           |\n",
            "------------------------------------------\n",
            "Episode 2724 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2725 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2726 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2727 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2728 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2729 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2730 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2731 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2732 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2733 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2734 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2735 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2736 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2737 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2738 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2739 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2740 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2741 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2742 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2743 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2744 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 134          |\n",
            "|    time_elapsed         | 2297         |\n",
            "|    total_timesteps      | 548864       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052603832 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.964       |\n",
            "|    explained_variance   | 0.0778       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 21.1         |\n",
            "|    n_updates            | 1330         |\n",
            "|    policy_gradient_loss | -0.00399     |\n",
            "|    value_loss           | 43           |\n",
            "------------------------------------------\n",
            "Episode 2745 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2746 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2747 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2748 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2749 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2750 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2751 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2752 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2753 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2754 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2755 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2756 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2757 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2758 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2759 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2760 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2761 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2762 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2763 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2764 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 2314         |\n",
            "|    total_timesteps      | 552960       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045013214 |\n",
            "|    clip_fraction        | 0.047        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.937       |\n",
            "|    explained_variance   | 0.0099       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 27           |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | -0.00478     |\n",
            "|    value_loss           | 62.2         |\n",
            "------------------------------------------\n",
            "Episode 2765 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2766 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2767 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2768 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2769 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2770 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2771 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2772 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2773 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2774 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2775 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2776 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2777 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2778 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2779 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2780 ended:\n",
            "  Reward: -220.0\n",
            "  Length: 200\n",
            "Episode 2781 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2782 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2783 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2784 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2785 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -123         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 2331         |\n",
            "|    total_timesteps      | 557056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059096725 |\n",
            "|    clip_fraction        | 0.0597       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0.00675      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 10.4         |\n",
            "|    n_updates            | 1350         |\n",
            "|    policy_gradient_loss | -0.00405     |\n",
            "|    value_loss           | 49.2         |\n",
            "------------------------------------------\n",
            "Episode 2786 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2787 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2788 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2789 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2790 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2791 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2792 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2793 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2794 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2795 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2796 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2797 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2798 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2799 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2800 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2801 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2802 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2803 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2804 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2805 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -124        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 238         |\n",
            "|    iterations           | 137         |\n",
            "|    time_elapsed         | 2348        |\n",
            "|    total_timesteps      | 561152      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004368767 |\n",
            "|    clip_fraction        | 0.0418      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.999      |\n",
            "|    explained_variance   | 0.0794      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 9.97        |\n",
            "|    n_updates            | 1360        |\n",
            "|    policy_gradient_loss | -0.00371    |\n",
            "|    value_loss           | 51.6        |\n",
            "-----------------------------------------\n",
            "Episode 2806 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2807 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2808 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2809 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2810 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2811 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2812 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2813 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2814 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2815 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2816 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2817 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2818 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2819 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2820 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2821 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2822 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2823 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2824 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 2825 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2826 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -124         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 138          |\n",
            "|    time_elapsed         | 2365         |\n",
            "|    total_timesteps      | 565248       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055611767 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0.0544       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 20           |\n",
            "|    n_updates            | 1370         |\n",
            "|    policy_gradient_loss | -0.00461     |\n",
            "|    value_loss           | 57.4         |\n",
            "------------------------------------------\n",
            "Episode 2827 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2828 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2829 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2830 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2831 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2832 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2833 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2834 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2835 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2836 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2837 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2838 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2839 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 2840 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2841 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2842 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 2843 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2844 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 2845 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2846 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -125         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 2382         |\n",
            "|    total_timesteps      | 569344       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050349254 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1           |\n",
            "|    explained_variance   | 0.0759       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 26.5         |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00394     |\n",
            "|    value_loss           | 53.7         |\n",
            "------------------------------------------\n",
            "Episode 2847 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2848 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2849 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2850 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2851 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 2852 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2853 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2854 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2855 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2856 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2857 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 2858 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 2859 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2860 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2861 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2862 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2863 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2864 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2865 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2866 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2867 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -124         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 238          |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 2399         |\n",
            "|    total_timesteps      | 573440       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050035673 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.982       |\n",
            "|    explained_variance   | 0.0158       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 34.9         |\n",
            "|    n_updates            | 1390         |\n",
            "|    policy_gradient_loss | -0.00365     |\n",
            "|    value_loss           | 53.9         |\n",
            "------------------------------------------\n",
            "Episode 2868 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2869 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2870 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2871 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2872 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2873 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2874 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2875 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2876 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2877 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2878 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2879 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 2880 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2881 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2882 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2883 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2884 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2885 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2886 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 2887 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -120        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 238         |\n",
            "|    iterations           | 141         |\n",
            "|    time_elapsed         | 2416        |\n",
            "|    total_timesteps      | 577536      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003989864 |\n",
            "|    clip_fraction        | 0.0424      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.883      |\n",
            "|    explained_variance   | 0.0216      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 1400        |\n",
            "|    policy_gradient_loss | -0.00431    |\n",
            "|    value_loss           | 51.6        |\n",
            "-----------------------------------------\n",
            "Episode 2888 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2889 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2890 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2891 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2892 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2893 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2894 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2895 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2896 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2897 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2898 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2899 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2900 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2901 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2902 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 2903 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 2904 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2905 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2906 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2907 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2908 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -120        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 142         |\n",
            "|    time_elapsed         | 2433        |\n",
            "|    total_timesteps      | 581632      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003593803 |\n",
            "|    clip_fraction        | 0.0387      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.754      |\n",
            "|    explained_variance   | 0.00359     |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 21.3        |\n",
            "|    n_updates            | 1410        |\n",
            "|    policy_gradient_loss | -0.00356    |\n",
            "|    value_loss           | 53.6        |\n",
            "-----------------------------------------\n",
            "Episode 2909 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2910 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2911 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2912 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2913 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2914 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2915 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2916 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2917 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2918 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2919 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2920 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2921 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2922 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2923 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2924 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2925 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2926 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2927 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2928 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -120       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 239        |\n",
            "|    iterations           | 143        |\n",
            "|    time_elapsed         | 2450       |\n",
            "|    total_timesteps      | 585728     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00370313 |\n",
            "|    clip_fraction        | 0.0414     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.869     |\n",
            "|    explained_variance   | 0.0124     |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 16.4       |\n",
            "|    n_updates            | 1420       |\n",
            "|    policy_gradient_loss | -0.00243   |\n",
            "|    value_loss           | 48.9       |\n",
            "----------------------------------------\n",
            "Episode 2929 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2930 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2931 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2932 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2933 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2934 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2935 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2936 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2937 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2938 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2939 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2940 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2941 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 2942 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2943 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2944 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2945 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2946 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2947 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2948 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2949 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -119        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 144         |\n",
            "|    time_elapsed         | 2467        |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004145409 |\n",
            "|    clip_fraction        | 0.0455      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.923      |\n",
            "|    explained_variance   | 0.0784      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 26.1        |\n",
            "|    n_updates            | 1430        |\n",
            "|    policy_gradient_loss | -0.00398    |\n",
            "|    value_loss           | 48.7        |\n",
            "-----------------------------------------\n",
            "Episode 2950 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2951 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2952 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 2953 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 2954 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2955 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2956 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 2957 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2958 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 2959 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 2960 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2961 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2962 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2963 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2964 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2965 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2966 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2967 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2968 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2969 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -117        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 145         |\n",
            "|    time_elapsed         | 2484        |\n",
            "|    total_timesteps      | 593920      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004522362 |\n",
            "|    clip_fraction        | 0.0412      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.917      |\n",
            "|    explained_variance   | 0.0306      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 9.72        |\n",
            "|    n_updates            | 1440        |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    value_loss           | 50.6        |\n",
            "-----------------------------------------\n",
            "Episode 2970 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2971 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 2972 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2973 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2974 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2975 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2976 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 2977 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2978 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 2979 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 2980 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2981 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 2982 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 2983 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 2984 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 2985 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2986 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 2987 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 2988 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2989 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 2990 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 146          |\n",
            "|    time_elapsed         | 2501         |\n",
            "|    total_timesteps      | 598016       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050212024 |\n",
            "|    clip_fraction        | 0.0547       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.956       |\n",
            "|    explained_variance   | 0.0541       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 13.2         |\n",
            "|    n_updates            | 1450         |\n",
            "|    policy_gradient_loss | -0.00426     |\n",
            "|    value_loss           | 47.8         |\n",
            "------------------------------------------\n",
            "Episode 2991 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 2992 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2993 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2994 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 2995 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 2996 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 2997 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2998 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 2999 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3000 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3001 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3002 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3003 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3004 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3005 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3006 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3007 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3008 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3009 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3010 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -119         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 147          |\n",
            "|    time_elapsed         | 2518         |\n",
            "|    total_timesteps      | 602112       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035663028 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.877       |\n",
            "|    explained_variance   | 0.0812       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 20.3         |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    value_loss           | 55           |\n",
            "------------------------------------------\n",
            "Episode 3011 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3012 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3013 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3014 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3015 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3016 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3017 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3018 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3019 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3020 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3021 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3022 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3023 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3024 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3025 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3026 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3027 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3028 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3029 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3030 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3031 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 148          |\n",
            "|    time_elapsed         | 2535         |\n",
            "|    total_timesteps      | 606208       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049917065 |\n",
            "|    clip_fraction        | 0.0514       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.04        |\n",
            "|    explained_variance   | 0.0332       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 27.2         |\n",
            "|    n_updates            | 1470         |\n",
            "|    policy_gradient_loss | -0.00416     |\n",
            "|    value_loss           | 48.8         |\n",
            "------------------------------------------\n",
            "Episode 3032 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3033 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3034 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3035 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3036 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3037 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3038 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3039 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3040 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3041 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3042 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3043 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3044 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3045 ended:\n",
            "  Reward: -205.0\n",
            "  Length: 200\n",
            "Episode 3046 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3047 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3048 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3049 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3050 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3051 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -122        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 149         |\n",
            "|    time_elapsed         | 2552        |\n",
            "|    total_timesteps      | 610304      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006347701 |\n",
            "|    clip_fraction        | 0.0578      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.969      |\n",
            "|    explained_variance   | 0.029       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 25.8        |\n",
            "|    n_updates            | 1480        |\n",
            "|    policy_gradient_loss | -0.0048     |\n",
            "|    value_loss           | 52.7        |\n",
            "-----------------------------------------\n",
            "Episode 3052 ended:\n",
            "  Reward: -225.0\n",
            "  Length: 200\n",
            "Episode 3053 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3054 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3055 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3056 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3057 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3058 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3059 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3060 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3061 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3062 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3063 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3064 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 3065 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3066 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3067 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3068 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3069 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3070 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3071 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3072 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 150          |\n",
            "|    time_elapsed         | 2568         |\n",
            "|    total_timesteps      | 614400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0049456013 |\n",
            "|    clip_fraction        | 0.0533       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.866       |\n",
            "|    explained_variance   | 0.0723       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 24.4         |\n",
            "|    n_updates            | 1490         |\n",
            "|    policy_gradient_loss | -0.00526     |\n",
            "|    value_loss           | 54.1         |\n",
            "------------------------------------------\n",
            "Episode 3073 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3074 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3075 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3076 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3077 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3078 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3079 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3080 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3081 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3082 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3083 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3084 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3085 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3086 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3087 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3088 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3089 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3090 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3091 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3092 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 151          |\n",
            "|    time_elapsed         | 2585         |\n",
            "|    total_timesteps      | 618496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051794527 |\n",
            "|    clip_fraction        | 0.0501       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.917       |\n",
            "|    explained_variance   | -0.0399      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 18.2         |\n",
            "|    n_updates            | 1500         |\n",
            "|    policy_gradient_loss | -0.00428     |\n",
            "|    value_loss           | 59.3         |\n",
            "------------------------------------------\n",
            "Episode 3093 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3094 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3095 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3096 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3097 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3098 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3099 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3100 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3101 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3102 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3103 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3104 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3105 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3106 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3107 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3108 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3109 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3110 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3111 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3112 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -122         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 152          |\n",
            "|    time_elapsed         | 2602         |\n",
            "|    total_timesteps      | 622592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042204103 |\n",
            "|    clip_fraction        | 0.0435       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.873       |\n",
            "|    explained_variance   | -0.00904     |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 43.4         |\n",
            "|    n_updates            | 1510         |\n",
            "|    policy_gradient_loss | -0.00337     |\n",
            "|    value_loss           | 52.5         |\n",
            "------------------------------------------\n",
            "Episode 3113 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3114 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3115 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3116 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3117 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3118 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3119 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3120 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 3121 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3122 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3123 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3124 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3125 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3126 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3127 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3128 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3129 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3130 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3131 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3132 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3133 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 153          |\n",
            "|    time_elapsed         | 2619         |\n",
            "|    total_timesteps      | 626688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052278377 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.989       |\n",
            "|    explained_variance   | -0.0153      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 34.3         |\n",
            "|    n_updates            | 1520         |\n",
            "|    policy_gradient_loss | -0.00435     |\n",
            "|    value_loss           | 54.9         |\n",
            "------------------------------------------\n",
            "Episode 3134 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3135 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3136 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3137 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3138 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3139 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3140 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3141 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3142 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3143 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3144 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3145 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3146 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3147 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3148 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3149 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3150 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3151 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3152 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3153 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -120       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 239        |\n",
            "|    iterations           | 154        |\n",
            "|    time_elapsed         | 2636       |\n",
            "|    total_timesteps      | 630784     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00597251 |\n",
            "|    clip_fraction        | 0.0549     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.905     |\n",
            "|    explained_variance   | 0.0398     |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 15.4       |\n",
            "|    n_updates            | 1530       |\n",
            "|    policy_gradient_loss | -0.00568   |\n",
            "|    value_loss           | 52.6       |\n",
            "----------------------------------------\n",
            "Episode 3154 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3155 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3156 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3157 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3158 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3159 ended:\n",
            "  Reward: -40.0\n",
            "  Length: 200\n",
            "Episode 3160 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3161 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3162 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3163 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3164 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3165 ended:\n",
            "  Reward: -185.0\n",
            "  Length: 200\n",
            "Episode 3166 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3167 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3168 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3169 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3170 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3171 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3172 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3173 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3174 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 200        |\n",
            "|    ep_rew_mean          | -122       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 239        |\n",
            "|    iterations           | 155        |\n",
            "|    time_elapsed         | 2653       |\n",
            "|    total_timesteps      | 634880     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00380855 |\n",
            "|    clip_fraction        | 0.0409     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.945     |\n",
            "|    explained_variance   | -0.0162    |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 12         |\n",
            "|    n_updates            | 1540       |\n",
            "|    policy_gradient_loss | -0.00318   |\n",
            "|    value_loss           | 50.1       |\n",
            "----------------------------------------\n",
            "Episode 3175 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3176 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3177 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3178 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3179 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3180 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3181 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3182 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3183 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3184 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3185 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3186 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3187 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3188 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3189 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3190 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3191 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3192 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3193 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3194 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -119        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 156         |\n",
            "|    time_elapsed         | 2670        |\n",
            "|    total_timesteps      | 638976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004422609 |\n",
            "|    clip_fraction        | 0.0551      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.969      |\n",
            "|    explained_variance   | 0.0489      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 1550        |\n",
            "|    policy_gradient_loss | -0.00528    |\n",
            "|    value_loss           | 51.4        |\n",
            "-----------------------------------------\n",
            "Episode 3195 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3196 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3197 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3198 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3199 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3200 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3201 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3202 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3203 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3204 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3205 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3206 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3207 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3208 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3209 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3210 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3211 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3212 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3213 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3214 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3215 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -119         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 157          |\n",
            "|    time_elapsed         | 2687         |\n",
            "|    total_timesteps      | 643072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045854645 |\n",
            "|    clip_fraction        | 0.054        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.02        |\n",
            "|    explained_variance   | 0.0416       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 18.2         |\n",
            "|    n_updates            | 1560         |\n",
            "|    policy_gradient_loss | -0.00426     |\n",
            "|    value_loss           | 49.8         |\n",
            "------------------------------------------\n",
            "Episode 3216 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3217 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 3218 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3219 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3220 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3221 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3222 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3223 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3224 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3225 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3226 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3227 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3228 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3229 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3230 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3231 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3232 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3233 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3234 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3235 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 158          |\n",
            "|    time_elapsed         | 2704         |\n",
            "|    total_timesteps      | 647168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059020324 |\n",
            "|    clip_fraction        | 0.0614       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.85        |\n",
            "|    explained_variance   | 0.0143       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 19.1         |\n",
            "|    n_updates            | 1570         |\n",
            "|    policy_gradient_loss | -0.00404     |\n",
            "|    value_loss           | 56.6         |\n",
            "------------------------------------------\n",
            "Episode 3236 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3237 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3238 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3239 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3240 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3241 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3242 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3243 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3244 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3245 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3246 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3247 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 3248 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3249 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3250 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3251 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3252 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3253 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3254 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3255 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3256 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 159          |\n",
            "|    time_elapsed         | 2721         |\n",
            "|    total_timesteps      | 651264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0063219946 |\n",
            "|    clip_fraction        | 0.0669       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.918       |\n",
            "|    explained_variance   | -0.0264      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 41.4         |\n",
            "|    n_updates            | 1580         |\n",
            "|    policy_gradient_loss | -0.00505     |\n",
            "|    value_loss           | 54.7         |\n",
            "------------------------------------------\n",
            "Episode 3257 ended:\n",
            "  Reward: -205.0\n",
            "  Length: 200\n",
            "Episode 3258 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3259 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "Episode 3260 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3261 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3262 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3263 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3264 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3265 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3266 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 3267 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3268 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3269 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3270 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3271 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3272 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3273 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3274 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3275 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3276 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -121        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 160         |\n",
            "|    time_elapsed         | 2738        |\n",
            "|    total_timesteps      | 655360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005205107 |\n",
            "|    clip_fraction        | 0.0623      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.861      |\n",
            "|    explained_variance   | 0.000864    |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 38.5        |\n",
            "|    n_updates            | 1590        |\n",
            "|    policy_gradient_loss | -0.00578    |\n",
            "|    value_loss           | 57.2        |\n",
            "-----------------------------------------\n",
            "Episode 3277 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3278 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3279 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3280 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3281 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3282 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3283 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3284 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3285 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3286 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3287 ended:\n",
            "  Reward: -65.0\n",
            "  Length: 200\n",
            "Episode 3288 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3289 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3290 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3291 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3292 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3293 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3294 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3295 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3296 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3297 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -121         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 161          |\n",
            "|    time_elapsed         | 2755         |\n",
            "|    total_timesteps      | 659456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051058475 |\n",
            "|    clip_fraction        | 0.0514       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.886       |\n",
            "|    explained_variance   | 0.0155       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 26           |\n",
            "|    n_updates            | 1600         |\n",
            "|    policy_gradient_loss | -0.00486     |\n",
            "|    value_loss           | 53.7         |\n",
            "------------------------------------------\n",
            "Episode 3298 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3299 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3300 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3301 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3302 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3303 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3304 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3305 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3306 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3307 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3308 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3309 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3310 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3311 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3312 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3313 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3314 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 3315 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3316 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3317 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -122         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 162          |\n",
            "|    time_elapsed         | 2772         |\n",
            "|    total_timesteps      | 663552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037893667 |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.879       |\n",
            "|    explained_variance   | 0.00767      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 27.2         |\n",
            "|    n_updates            | 1610         |\n",
            "|    policy_gradient_loss | -0.00344     |\n",
            "|    value_loss           | 54.5         |\n",
            "------------------------------------------\n",
            "Episode 3318 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3319 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3320 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3321 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3322 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3323 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3324 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3325 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3326 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3327 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3328 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3329 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3330 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3331 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3332 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3333 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3334 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3335 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3336 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3337 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3338 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -121        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 163         |\n",
            "|    time_elapsed         | 2789        |\n",
            "|    total_timesteps      | 667648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002998265 |\n",
            "|    clip_fraction        | 0.0345      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.88       |\n",
            "|    explained_variance   | 0.044       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 39.3        |\n",
            "|    n_updates            | 1620        |\n",
            "|    policy_gradient_loss | -0.00311    |\n",
            "|    value_loss           | 51.4        |\n",
            "-----------------------------------------\n",
            "Episode 3339 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3340 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3341 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3342 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3343 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3344 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3345 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3346 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3347 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3348 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3349 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3350 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3351 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3352 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 3353 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3354 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3355 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3356 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3357 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3358 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -122         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 164          |\n",
            "|    time_elapsed         | 2806         |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030359626 |\n",
            "|    clip_fraction        | 0.0303       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.908       |\n",
            "|    explained_variance   | -0.00677     |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 44.9         |\n",
            "|    n_updates            | 1630         |\n",
            "|    policy_gradient_loss | -0.00246     |\n",
            "|    value_loss           | 56.7         |\n",
            "------------------------------------------\n",
            "Episode 3359 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3360 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3361 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3362 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3363 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "Episode 3364 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3365 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 3366 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3367 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3368 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3369 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3370 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 3371 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3372 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3373 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3374 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3375 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3376 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3377 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3378 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3379 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -123         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 165          |\n",
            "|    time_elapsed         | 2823         |\n",
            "|    total_timesteps      | 675840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050898427 |\n",
            "|    clip_fraction        | 0.0508       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.831       |\n",
            "|    explained_variance   | -0.0133      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 16.5         |\n",
            "|    n_updates            | 1640         |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 52.1         |\n",
            "------------------------------------------\n",
            "Episode 3380 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3381 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3382 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3383 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3384 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3385 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3386 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3387 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3388 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3389 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3390 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 3391 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3392 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3393 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3394 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3395 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3396 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3397 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3398 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3399 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -123         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 166          |\n",
            "|    time_elapsed         | 2840         |\n",
            "|    total_timesteps      | 679936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035547311 |\n",
            "|    clip_fraction        | 0.0534       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.758       |\n",
            "|    explained_variance   | 0.0104       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 28.8         |\n",
            "|    n_updates            | 1650         |\n",
            "|    policy_gradient_loss | -0.00423     |\n",
            "|    value_loss           | 65           |\n",
            "------------------------------------------\n",
            "Episode 3400 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3401 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 3402 ended:\n",
            "  Reward: -220.0\n",
            "  Length: 200\n",
            "Episode 3403 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3404 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3405 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3406 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3407 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3408 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3409 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3410 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3411 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3412 ended:\n",
            "  Reward: -105.0\n",
            "  Length: 200\n",
            "Episode 3413 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3414 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3415 ended:\n",
            "  Reward: -60.0\n",
            "  Length: 200\n",
            "Episode 3416 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3417 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3418 ended:\n",
            "  Reward: -155.0\n",
            "  Length: 200\n",
            "Episode 3419 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3420 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 200         |\n",
            "|    ep_rew_mean          | -122        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 239         |\n",
            "|    iterations           | 167         |\n",
            "|    time_elapsed         | 2857        |\n",
            "|    total_timesteps      | 684032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004191845 |\n",
            "|    clip_fraction        | 0.0452      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.773      |\n",
            "|    explained_variance   | 0.0175      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 22.2        |\n",
            "|    n_updates            | 1660        |\n",
            "|    policy_gradient_loss | -0.00376    |\n",
            "|    value_loss           | 48.5        |\n",
            "-----------------------------------------\n",
            "Episode 3421 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3422 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3423 ended:\n",
            "  Reward: -165.0\n",
            "  Length: 200\n",
            "Episode 3424 ended:\n",
            "  Reward: -125.0\n",
            "  Length: 200\n",
            "Episode 3425 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3426 ended:\n",
            "  Reward: -95.0\n",
            "  Length: 200\n",
            "Episode 3427 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3428 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3429 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3430 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3431 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3432 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3433 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3434 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3435 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3436 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3437 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3438 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3439 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3440 ended:\n",
            "  Reward: -145.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -120         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 168          |\n",
            "|    time_elapsed         | 2874         |\n",
            "|    total_timesteps      | 688128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032287878 |\n",
            "|    clip_fraction        | 0.0329       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.833       |\n",
            "|    explained_variance   | -0.0453      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 33.1         |\n",
            "|    n_updates            | 1670         |\n",
            "|    policy_gradient_loss | -0.00264     |\n",
            "|    value_loss           | 52.8         |\n",
            "------------------------------------------\n",
            "Episode 3441 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3442 ended:\n",
            "  Reward: -90.0\n",
            "  Length: 200\n",
            "Episode 3443 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3444 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3445 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3446 ended:\n",
            "  Reward: -85.0\n",
            "  Length: 200\n",
            "Episode 3447 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3448 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3449 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3450 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3451 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3452 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3453 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3454 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3455 ended:\n",
            "  Reward: -210.0\n",
            "  Length: 200\n",
            "Episode 3456 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3457 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3458 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3459 ended:\n",
            "  Reward: -30.0\n",
            "  Length: 200\n",
            "Episode 3460 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3461 ended:\n",
            "  Reward: -175.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -118         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 169          |\n",
            "|    time_elapsed         | 2891         |\n",
            "|    total_timesteps      | 692224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040001427 |\n",
            "|    clip_fraction        | 0.0465       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.855       |\n",
            "|    explained_variance   | 0.068        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 19.7         |\n",
            "|    n_updates            | 1680         |\n",
            "|    policy_gradient_loss | -0.00339     |\n",
            "|    value_loss           | 48.5         |\n",
            "------------------------------------------\n",
            "Episode 3462 ended:\n",
            "  Reward: -135.0\n",
            "  Length: 200\n",
            "Episode 3463 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3464 ended:\n",
            "  Reward: -200.0\n",
            "  Length: 200\n",
            "Episode 3465 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3466 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3467 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3468 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3469 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3470 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3471 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3472 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3473 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3474 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3475 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3476 ended:\n",
            "  Reward: -115.0\n",
            "  Length: 200\n",
            "Episode 3477 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "Episode 3478 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3479 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3480 ended:\n",
            "  Reward: -190.0\n",
            "  Length: 200\n",
            "Episode 3481 ended:\n",
            "  Reward: -80.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -117         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 170          |\n",
            "|    time_elapsed         | 2908         |\n",
            "|    total_timesteps      | 696320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052601867 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.794       |\n",
            "|    explained_variance   | 0.0989       |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 21.7         |\n",
            "|    n_updates            | 1690         |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    value_loss           | 51.4         |\n",
            "------------------------------------------\n",
            "Episode 3482 ended:\n",
            "  Reward: -180.0\n",
            "  Length: 200\n",
            "Episode 3483 ended:\n",
            "  Reward: -50.0\n",
            "  Length: 200\n",
            "Episode 3484 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3485 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3486 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3487 ended:\n",
            "  Reward: -75.0\n",
            "  Length: 200\n",
            "Episode 3488 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3489 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3490 ended:\n",
            "  Reward: -110.0\n",
            "  Length: 200\n",
            "Episode 3491 ended:\n",
            "  Reward: -70.0\n",
            "  Length: 200\n",
            "Episode 3492 ended:\n",
            "  Reward: -120.0\n",
            "  Length: 200\n",
            "Episode 3493 ended:\n",
            "  Reward: -215.0\n",
            "  Length: 200\n",
            "Episode 3494 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "Episode 3495 ended:\n",
            "  Reward: -160.0\n",
            "  Length: 200\n",
            "Episode 3496 ended:\n",
            "  Reward: -150.0\n",
            "  Length: 200\n",
            "Episode 3497 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3498 ended:\n",
            "  Reward: -100.0\n",
            "  Length: 200\n",
            "Episode 3499 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3500 ended:\n",
            "  Reward: -140.0\n",
            "  Length: 200\n",
            "Episode 3501 ended:\n",
            "  Reward: -130.0\n",
            "  Length: 200\n",
            "Episode 3502 ended:\n",
            "  Reward: -170.0\n",
            "  Length: 200\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 200          |\n",
            "|    ep_rew_mean          | -118         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 239          |\n",
            "|    iterations           | 171          |\n",
            "|    time_elapsed         | 2925         |\n",
            "|    total_timesteps      | 700416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039036763 |\n",
            "|    clip_fraction        | 0.0441       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.765       |\n",
            "|    explained_variance   | -0.0305      |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 20.5         |\n",
            "|    n_updates            | 1700         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 56.6         |\n",
            "------------------------------------------\n",
            "\n",
            "==== Training Summary ====\n",
            "Total Episodes: 3502\n",
            "Average Reward per Episode: -121.78\n",
            "Average Episode Length: 200.00\n",
            "Final Episode Reward: -170.00\n",
            "==========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ohBpx28k3tWp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#====================Evaluating and recording video======\n",
        "\n",
        "from stable_baselines3 import DQN, PPO\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "# from environment.custom_env import EpilepsyDetectionEnv\n",
        "\n",
        "model_dqn = DQN.load(\"epilepsy_dqn_model.zip\")\n",
        "model_ppo = PPO.load(\"epilepsy_ppo_model.zip\")\n",
        "\n",
        "env_dqn = RecordVideo(\n",
        "    EpilepsyDetectionEnv(render_mode='rgb_array'),\n",
        "    video_folder='rl_agent_videos_dqn',\n",
        "    episode_trigger=lambda x: True\n",
        ")\n",
        "\n",
        "env_ppo = RecordVideo(\n",
        "    EpilepsyDetectionEnv(render_mode='rgb_array'),\n",
        "    video_folder='rl_agent_videos_ppo',\n",
        "    episode_trigger=lambda x: True\n",
        ")\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_and_record(model, env, episodes=20, model_name='model'):\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        total_reward = 0\n",
        "        for step in range(200):\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        print(f\"{model_name} Episode {episode+1}: Total Reward = {total_reward:.2f}\")\n",
        "    env.close()\n",
        "\n",
        "# Record videos clearly\n",
        "evaluate_and_record(model_dqn, env_dqn, model_name='DQN')\n",
        "evaluate_and_record(model_ppo, env_ppo, model_name='PPO')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm8j2gsCNVtb",
        "outputId": "cd1d21c9-c1b7-49a1-aebc-54c759054b38"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DQN Episode 1: Total Reward = -110.00\n",
            "DQN Episode 2: Total Reward = -110.00\n",
            "DQN Episode 3: Total Reward = -90.00\n",
            "DQN Episode 4: Total Reward = -160.00\n",
            "DQN Episode 5: Total Reward = -120.00\n",
            "DQN Episode 6: Total Reward = -130.00\n",
            "DQN Episode 7: Total Reward = -80.00\n",
            "DQN Episode 8: Total Reward = -120.00\n",
            "DQN Episode 9: Total Reward = -135.00\n",
            "DQN Episode 10: Total Reward = -160.00\n",
            "DQN Episode 11: Total Reward = -125.00\n",
            "DQN Episode 12: Total Reward = -170.00\n",
            "DQN Episode 13: Total Reward = -100.00\n",
            "DQN Episode 14: Total Reward = -100.00\n",
            "DQN Episode 15: Total Reward = -160.00\n",
            "DQN Episode 16: Total Reward = -150.00\n",
            "DQN Episode 17: Total Reward = -140.00\n",
            "DQN Episode 18: Total Reward = -105.00\n",
            "DQN Episode 19: Total Reward = -170.00\n",
            "DQN Episode 20: Total Reward = -90.00\n",
            "PPO Episode 1: Total Reward = -90.00\n",
            "PPO Episode 2: Total Reward = -120.00\n",
            "PPO Episode 3: Total Reward = -105.00\n",
            "PPO Episode 4: Total Reward = -110.00\n",
            "PPO Episode 5: Total Reward = -150.00\n",
            "PPO Episode 6: Total Reward = -105.00\n",
            "PPO Episode 7: Total Reward = -150.00\n",
            "PPO Episode 8: Total Reward = -140.00\n",
            "PPO Episode 9: Total Reward = -150.00\n",
            "PPO Episode 10: Total Reward = -120.00\n",
            "PPO Episode 11: Total Reward = -120.00\n",
            "PPO Episode 12: Total Reward = -90.00\n",
            "PPO Episode 13: Total Reward = -60.00\n",
            "PPO Episode 14: Total Reward = -115.00\n",
            "PPO Episode 15: Total Reward = -130.00\n",
            "PPO Episode 16: Total Reward = -90.00\n",
            "PPO Episode 17: Total Reward = -130.00\n",
            "PPO Episode 18: Total Reward = -120.00\n",
            "PPO Episode 19: Total Reward = -140.00\n",
            "PPO Episode 20: Total Reward = -85.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1Gv4lZpPVuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}